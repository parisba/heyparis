---
title: "What the AISI Job Ads Tell Us About Australia's AI Safety Priorities"
draft: true
date: 2026-01-07
cover:
  image: "cover.jpg"
  alt: "What the AISI Job Ads Tell Us About Australia's AI Safety Priorities"
  relative: true
ShowToc: false
params:
  description: A close reading of the Australian AI Safety Institute's job advertisements reveals what kind of organisation the government is actually building.
  images:
  - cover.jpg
  title: "What the AISI Job Ads Tell Us About Australia's AI Safety Priorities"
tags: ["ai", "ai safety", "australia", "aisi", "jobs", "policy", "analysis", "recruitment"]
---

Job advertisements are revealing documents. They tell you what an organisation actually values, not what it says it values. They show you who they want to hire, which shapes who they will become.

The [Australian AI Safety Institute](https://www.industry.gov.au/news/australia-establishes-new-institute-strengthen-ai-safety) (AISI) is currently recruiting its founding team. Applications close 18 January 2026. The job ads are public, and they paint an interesting picture of what Australia is building.

Let's read between the lines.

## The Advertised Roles

From the [Department of Industry, Science and Resources careers portal](https://www.industry.gov.au/news/australia-establishes-new-institute-strengthen-ai-safety) and [Good Ancestors' recruitment support page](https://www.goodancestors.org.au/aisi-founders), the AISI is recruiting for:

- AI Risk Specialists
- Research Scientists
- Engineers
- Senior leadership positions

The AI Risk Specialist job ad (EL1 level) is the most detailed publicly available, so let's examine it closely.

## What They're Looking For

The [AI Risk Specialist role](https://www.industry.gov.au/news/australia-establishes-new-institute-strengthen-ai-safety) asks for candidates who can:

> "Assess risks from frontier models, including CBRN misuse, enhanced cyber capabilities, loss-of-control scenarios, information integrity and influence risks, and broader systemic risks arising from the deployment of increasingly capable general-purpose AI systems."

This is a comprehensive list. It covers:

- **CBRN risks** (chemical, biological, radiological, nuclear): Can AI help bad actors develop weapons?
- **Cyber capabilities**: Can AI enable more sophisticated cyberattacks?
- **Loss-of-control scenarios**: The technical AI safety concern about systems pursuing goals misaligned with human intentions
- **Information integrity**: Misinformation, deepfakes, AI-generated propaganda
- **Systemic deployment risks**: What happens when AI is embedded across critical systems?

This suggests the AISI is taking a broad view of AI safety, not just narrow technical alignment questions, but also misuse and societal deployment risks.

## The Desired Experience

The "ideal candidate" section is instructive:

- "Experience working in AI safety, security or governance in industry, academia, government or independently"
- "Demonstrable knowledge of technical AI governance matters, including general-purpose AI risk assessment methodologies, scenario-based risk modelling, governance and control protocols"
- "Demonstrated experience synthesising technical AI safety research into actionable guidance for decision-makers"
- "A broad understanding of LLMs and the technical drivers that shape their behaviour, safety properties and risk profiles"

This is seeking a hybrid profile: technical knowledge of AI systems combined with policy translation skills. Someone who understands both how LLMs work and how government decisions get made.

It's an unusual combination. Most AI safety researchers don't have policy experience. Most policy professionals don't have deep technical knowledge of LLMs. Finding candidates who can do both will be challenging.

## The UK and US Comparison

How does this compare to what the [UK AI Security Institute](https://www.aisi.gov.uk/) and (former) [US AI Safety Institute](https://www.nist.gov/artificial-intelligence/us-ai-safety-institute-aisic) were looking for?

**UK AISI:** Recruited "over 30 technical staff, including senior alumni from OpenAI, Google DeepMind and the University of Oxford". Their focus was heavily technical: building evaluation frameworks, running model tests, publishing research with empirical findings. Their [Inspect platform](https://inspect.aisi.org.uk/) required software engineers who could build production-quality open-source tools.

**US AISI:** Before its transformation into CAISI, recruited through a large consortium model, bringing together over 290 organisations. The focus was on standards development and industry engagement as much as technical evaluation.

**Australia:** Appears to be seeking a middle ground. Technical capability in AI systems, but with explicit policy translation skills. Less focus on building tools, more focus on advising government.

## What's Missing

The job ads don't mention:

**Australian context expertise.** There's no explicit requirement for understanding Australian legal frameworks, Indigenous data sovereignty, or Australian-specific deployment contexts. Given that Australia's AI challenges are partly about importing systems designed for other contexts, this seems like a gap.

**Evaluation infrastructure.** The UK built Inspect. There's no indication Australia plans to build equivalent technical infrastructure. The AISI may be relying on international tools rather than developing its own.

**Algorithmic auditing.** The roles focus on frontier models and general-purpose AI. There's less obvious attention to the kinds of narrow AI systems already deployed in Australian government (fraud detection, eligibility assessment, risk scoring).

**Community engagement.** No mention of working with affected communities, civil society organisations, or the people who experience AI system harms. The focus is on advising government and engaging with industry.

## The International Engagement Focus

The job ads emphasise:

> "Collaborate with domestic and international partners across governments, industry, civil society and academia to strengthen AI safety governance practices."

And:

> "Represent Australia in international AI safety governance collaborations."

This positions the AISI as primarily an international engagement body. Its value comes from participating in global conversations, not from building unique domestic capability.

That's not necessarily wrong. For a country without frontier AI labs, contributing to international networks may be the highest-value approach. But it does raise questions about what distinctive capability Australia is actually building.

If the AISI's main function is to participate in international forums and translate international research for Australian policymakers, that's valuable but limited. It's a receiving antenna, not a transmitting station.

## The Salary Reality

The [advertised salary](https://www.industry.gov.au/news/australia-establishes-new-institute-strengthen-ai-safety) for the EL1 AI Risk Specialist is $122,235 to $129,811.

For comparison:
- An AI safety researcher at a major lab earns significantly more
- A senior ML engineer in Australian private sector earns significantly more
- Even mid-level policy professionals in consulting earn comparable amounts

This salary will not attract people from industry labs. It might attract academics, people from NGOs, or people willing to take a pay cut for public service. The pool of candidates who have frontier AI technical knowledge AND policy skills AND are willing to work for APS salaries is very small.

[Good Ancestors notes](https://www.goodancestors.org.au/aisi-founders): "Talent pipelines into AI safety roles in places like the US and UK are increasingly established. Australia's is much thinner."

This is diplomatically stated. The reality is that Australia will struggle to recruit the calibre of candidates the UK recruited from DeepMind and OpenAI.

## The Security Clearance Requirement

The roles "require the ability to obtain a minimum baseline security clearance, and the ability to obtain higher security clearance as required."

This is standard for government positions. But it creates constraints:

- Non-citizens are ineligible
- People with certain international connections may face delays or refusals
- The clearance process itself takes months

Many AI safety researchers with the desired experience are internationally mobile. They may be Australian citizens who've lived overseas for years, or they may have worked at labs in multiple countries. The clearance process may filter out some strong candidates.

## What This Suggests

Reading the job ads together, the AISI appears to be:

1. **Generalist, not specialist.** Broad risk coverage rather than deep technical capability in any one area.

2. **Advisory, not operational.** Focus on providing advice to government, not building evaluation infrastructure.

3. **Internationally oriented.** Significant emphasis on participation in global networks.

4. **Resource-constrained.** Salaries won't attract industry talent; small team expected.

5. **Policy-focused.** Seeking the rare hybrid of technical knowledge and policy skills.

This isn't necessarily wrong. Given Australia's constraints (no frontier labs, limited budget, late start), this approach may be pragmatic.

But it's different from what the UK built. The UK AISI is a research organisation that produces technical outputs: tools, evaluations, published findings. The Australian AISI appears to be more like a policy advisory body that consumes international research and translates it for domestic use.

## The Opportunity

Despite the constraints, [Good Ancestors makes a good point](https://www.goodancestors.org.au/aisi-founders):

> "If you're eligible and a good fit for a role at the AISI, the gap between you and the next-best candidate is likely larger than it would be in established ecosystems."

Translation: because the Australian talent pool is thin, individual candidates can have outsized impact.

If you're an Australian with relevant skills, the AISI offers an unusual opportunity. You won't be one of dozens of similar candidates competing for a position. You might be one of a handful.

Whether that's appealing depends on whether you believe the AISI will actually be able to do meaningful work, or whether it'll become another advisory body producing reports nobody reads.

## The Questions Remaining

The job ads don't tell us:

- How many positions total?
- What's the overall AISI budget and staffing plan?
- What specific projects will the team work on?
- How will the AISI relate to the AI Review Committee and other governance bodies?
- What authority will the AISI have to influence agency AI deployments?

The AISI is scheduled to be operational in "early 2026". With applications closing 18 January 2026, the timeline is extremely tight. First hires will be onboarding into an organisation that's still being defined.

That's either exciting or concerning, depending on your appetite for ambiguity.

---

*The AISI job ads are available through the [Department of Industry careers portal](https://www.industry.gov.au/news/australia-establishes-new-institute-strengthen-ai-safety). [Good Ancestors](https://www.goodancestors.org.au/aisi-founders) has useful context for prospective applicants. Applications close 18 January 2026.*
